{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-11-18T15:13:37.411222Z","iopub.status.busy":"2024-11-18T15:13:37.410143Z","iopub.status.idle":"2024-11-18T15:14:14.562086Z","shell.execute_reply":"2024-11-18T15:14:14.560792Z","shell.execute_reply.started":"2024-11-18T15:13:37.411175Z"},"trusted":true},"outputs":[],"source":["# !pip install einops\n","# !pip install monai==1.3.2\n","# !pip install rasterio"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-18T15:14:14.564590Z","iopub.status.busy":"2024-11-18T15:14:14.564239Z","iopub.status.idle":"2024-11-18T15:14:56.737161Z","shell.execute_reply":"2024-11-18T15:14:56.736303Z","shell.execute_reply.started":"2024-11-18T15:14:14.564554Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import os\n","import glob\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler\n","from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n","import pandas as pd\n","from sklearn.impute import SimpleImputer\n","import torch\n","from sklearn.model_selection import KFold\n","from monai.networks.nets import SwinUNETR, DynUNet, UNet\n","from monai.transforms import RandFlipd, RandRotate90d\n","from torch import nn\n","from transformers import get_cosine_schedule_with_warmup\n","from sklearn.metrics import accuracy_score\n","import torch.optim as optim\n","from tqdm import tqdm\n","import gc\n","import rasterio\n","from monai.losses import DiceLoss\n","from PIL import Image\n","import torchvision.transforms as transforms\n","from torchvision.transforms.v2 import GaussianNoise\n","import random\n","from monai.inferers import SlidingWindowInferer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-18T15:14:56.739010Z","iopub.status.busy":"2024-11-18T15:14:56.738322Z","iopub.status.idle":"2024-11-18T15:14:56.749448Z","shell.execute_reply":"2024-11-18T15:14:56.748499Z","shell.execute_reply.started":"2024-11-18T15:14:56.738973Z"},"trusted":true},"outputs":[],"source":["class CustomImageDataset(Dataset):\n","    def __init__(self, data_list, transform=True):\n","        self.transform   = transform\n","        self.data = data_list\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        image = Image.fromarray(self.data[idx][:4].transpose([1, 2, 0])).convert('RGB')\n","        image = transforms.ToTensor()(image)\n","        label    = torch.tensor(self.data[idx][4:])\n","        if self.transform:\n","            image_label = {'image': image, 'label': label}\n","            image_label = RandFlipd(keys=['image', 'label'], spatial_axis=1)(image_label)\n","            image_label = RandFlipd(keys=['image', 'label'], spatial_axis=0)(image_label)\n","            image_label = RandRotate90d(keys=['image', 'label'], spatial_axes=(0, 1))(image_label)\n","            image, label = image_label['image'], image_label['label']\n","            if random.random() < 0.1:\n","                image = GaussianNoise(sigma=0.01)(image)\n","            if random.random() < 0.1:\n","                image = transforms.GaussianBlur(5)(image)\n","            if random.random() < 0.1:\n","                image = transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1)(image)         \n","            \n","        return image, label"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-18T15:14:56.753200Z","iopub.status.busy":"2024-11-18T15:14:56.752911Z","iopub.status.idle":"2024-11-18T15:14:56.766190Z","shell.execute_reply":"2024-11-18T15:14:56.765350Z","shell.execute_reply.started":"2024-11-18T15:14:56.753169Z"},"trusted":true},"outputs":[],"source":["def create_dataloaders(data_list, batch_size=32, n_fold=0):\n","    # Initialize dataset\n","    dataset = CustomImageDataset(data_list=data_list, transform=True)\n","    \n","    # Create train/validation split\n","    kf = KFold(n_splits=5, shuffle=True, random_state=2024)\n","    for i, (train_index, val_index) in enumerate(kf.split(dataset)):\n","        if i == n_fold:\n","            break\n","            \n","    train_dataset = Subset(dataset, train_index)\n","    dataset = CustomImageDataset(data_list=data_list, transform=False)\n","    val_dataset = Subset(dataset, val_index)\n","    print(len(train_dataset))\n","    print(len(val_dataset))\n","\n","    # Create DataLoaders\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n","    \n","    return train_loader, val_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-18T15:14:56.768147Z","iopub.status.busy":"2024-11-18T15:14:56.767406Z","iopub.status.idle":"2024-11-18T15:14:56.779256Z","shell.execute_reply":"2024-11-18T15:14:56.778406Z","shell.execute_reply.started":"2024-11-18T15:14:56.768077Z"},"trusted":true},"outputs":[],"source":["class Loss(nn.Module):\n","    def __init__(self):\n","        super(Loss, self).__init__()\n","        self.dice = DiceLoss(to_onehot_y=True, softmax=True, batch=True)\n","        self.ce = nn.CrossEntropyLoss()\n","\n","    def forward(self, p, y):\n","#         return self.ce(p, y[:, 0, ...]) + self.dice(p, y)\n","        return self.ce(p, y[:, 0, ...])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-18T15:17:12.637525Z","iopub.status.busy":"2024-11-18T15:17:12.636614Z","iopub.status.idle":"2024-11-18T15:17:12.651581Z","shell.execute_reply":"2024-11-18T15:17:12.650342Z","shell.execute_reply.started":"2024-11-18T15:17:12.637481Z"},"trusted":true},"outputs":[],"source":["def train_one_epoch(model, train_loader, criterion, optimizer, scheduler, device, deep_supervision=False):\n","    model.train()\n","    running_loss = 0.0\n","    for images, labels in tqdm(train_loader):\n","        images, labels = images.to(device).float(), labels.to(device).long()\n","\n","        optimizer.zero_grad()\n","        \n","        outputs = model(images)\n","        if deep_supervision:\n","            major_loss = criterion(outputs[:, 0, ...], labels)\n","            moderate_loss = criterion(outputs[:, 1, ...], labels)\n","            minor_loss = criterion(outputs[:, 2, ...], labels)\n","            loss = major_loss + 0.5 * moderate_loss + 0.25 * minor_loss\n","        else:\n","            loss = criterion(outputs, labels)\n","\n","        # Backward pass and optimization step\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    return epoch_loss\n","\n","def validate(model, val_loader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    running_true = 0.0\n","    running_true_false = 0.0\n","#     all_labels = []\n","#     all_outputs = []\n","    \n","    with torch.no_grad():\n","        for images, labels in tqdm(val_loader):\n","            images, labels = images.to(device).float(), labels.to(device).long()\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            running_loss += loss.item() * images.size(0)\n","            outputs = torch.softmax(outputs, dim=1).cpu().numpy()\n","            labels = labels.cpu().numpy()\n","            preds = outputs.argmax(axis=1, keepdims=True)\n","            running_true += accuracy_score(labels.reshape(-1), preds.reshape(-1), normalize=False) * 0.6 + accuracy_score((labels == 0).astype(int).reshape(-1), (preds == 0).astype(int).reshape(-1), normalize=False) * 0.4\n","            running_true_false  += len(labels.reshape(-1))\n","#             all_labels.append(labels.cpu().numpy())\n","#             all_outputs.append(outputs.cpu().numpy())\n","\n","    epoch_loss = running_loss / len(val_loader.dataset)\n","    epoch_metric = running_true / running_true_false\n","    \n","#     all_labels = np.concatenate(all_labels)\n","#     all_outputs = np.concatenate(all_outputs)\n","#     all_outputs = torch.softmax(torch.tensor(all_outputs), dim=1).numpy()  # Convert logits to probabilities\n","    \n","    return epoch_loss, epoch_metric"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-18T15:14:56.797686Z","iopub.status.busy":"2024-11-18T15:14:56.797016Z","iopub.status.idle":"2024-11-18T15:14:56.807949Z","shell.execute_reply":"2024-11-18T15:14:56.807085Z","shell.execute_reply.started":"2024-11-18T15:14:56.797644Z"},"trusted":true},"outputs":[],"source":["class EarlyStopping:\n","    def __init__(self, patience=7, verbose=False, delta=0):\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","\n","    def __call__(self, val_loss, model, path):\n","        score = -val_loss\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model, path)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model, path)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model, path):\n","        if self.verbose:\n","            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n","        self.val_loss_min = val_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-18T15:17:24.695740Z","iopub.status.busy":"2024-11-18T15:17:24.695006Z","iopub.status.idle":"2024-11-18T15:17:24.706435Z","shell.execute_reply":"2024-11-18T15:17:24.705420Z","shell.execute_reply.started":"2024-11-18T15:17:24.695694Z"},"trusted":true},"outputs":[],"source":["def train_model(data_list, model, model_name, num_epochs=10, batch_size=32, lr=1e-4, n_fold=0, device='cuda', patience=3, warmup_epochs=0, \n","               deep_supervision=False):\n","    train_loader, val_loader = create_dataloaders(data_list=data_list, batch_size=batch_size, n_fold=n_fold)\n","    train_sets = len(train_loader)\n","\n","    model     = model.to(device)\n","    criterion = Loss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    scheduler = get_cosine_schedule_with_warmup(optimizer, train_sets * warmup_epochs, train_sets * num_epochs)\n","    early_stopping = EarlyStopping(patience=patience, verbose=True)\n","\n","    train_losses = []\n","    val_losses   = []\n","    \n","    path = model_name + str(n_fold)\n","    os.makedirs(path, exist_ok=True)\n","    \n","    for epoch in range(num_epochs):\n","        print(f'Epoch {epoch+1}/{num_epochs}')\n","        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, scheduler, device, deep_supervision)\n","        val_loss, accuracy = validate(model, val_loader, criterion, device)\n","\n","        train_losses.append(train_loss)\n","        val_losses.append(val_loss)\n","        \n","        # Calculate metrics on validation set\n","        print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {accuracy:.4f}')\n","        early_stopping(-accuracy, model, path)\n","        if early_stopping.early_stop:\n","            print(\"Early stopping\")\n","            break\n","        print('Updating learning rate to {}'.format(scheduler.get_last_lr()[0]))\n","        \n","    # Plot Loss\n","    plt.plot(train_losses, label='Train Loss')\n","    plt.plot(val_losses, label='Validation Loss')\n","    plt.legend()\n","    plt.savefig('loss_plot.png')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# TODO  \n","- data agumentation  \n","- deep supervision\n","- test time agumentation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-18T15:17:29.293723Z","iopub.status.busy":"2024-11-18T15:17:29.292925Z"},"trusted":true},"outputs":[],"source":["data = np.load('/kaggle/input/large-to-small-image/features_label.npy')\n","gc.collect()\n","for n_fold in range(2):\n","    model = DynUNet(\n","        spatial_dims=2,\n","        in_channels=3,\n","        out_channels=3,\n","        kernel_size=(3, 3, 3, 3, 3, 3, 3),\n","        strides=(1, 2, 2, 2, 2, 2, 2),\n","        upsample_kernel_size=(2, 2, 2, 2, 2, 2),\n","        deep_supervision=True, \n","        deep_supr_num=2, \n","#         dropout=0.0, \n","        filters=[64, 96, 128, 192, 256, 384, 512]\n","    )\n","    batch_size = 64\n","    train_model(data, model, 'DynUNet', num_epochs=10, batch_size=batch_size, \n","                lr=1e-3, n_fold=n_fold, device='cuda', patience=5, warmup_epochs=1, deep_supervision=True)\n","    torch.cuda.empty_cache()\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6105762,"sourceId":9932598,"sourceType":"datasetVersion"},{"sourceId":208180731,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
